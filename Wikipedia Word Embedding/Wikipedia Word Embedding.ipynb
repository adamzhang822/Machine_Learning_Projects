{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import scipy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I couldn't download nltk stopwords because python keeps giving me errors, so I manually downloaded the txt file\n",
    "# from nltk website and manually read it\n",
    "wiki_full = open('wiki-text.txt',\"r\").readlines()[0].split()\n",
    "stopwords = open('english.txt',\"r\").readlines()\n",
    "stopwords = [s.replace('\\n','') for s in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8124"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_sub = wiki_full[0:int(len(wiki_full)*(3/7))] # I use 3/7th of the wiki corpus \n",
    "counts = dict(collections.Counter(wiki_sub))\n",
    "counts = {key:value for (key,value) in counts.items() if value >= 500} # filter out infrequent words\n",
    "counts = {key:value for (key,value) in counts.items() if key not in stopwords} # filuter out stopwords\n",
    "len(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to computing limitation my vocab size is around 8000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabs = list(counts.keys())\n",
    "vocabs_dict = dict(enumerate(vocabs))\n",
    "vocabs_dict = dict((v,k) for k,v in vocabs_dict.items()) # this dictionary will act as indexes for PMI-matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "# create a dictionary that tracks list of all occurrences of a word in corpus\n",
    "corpus_dict = defaultdict(list) \n",
    "for index,word in enumerate(wiki_sub):\n",
    "    if index % 1000000 ==0:\n",
    "        print(index)\n",
    "    if word in vocabs:\n",
    "        corpus_dict[word].append(index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "radius = list(range(-5,0)) + list(range(1,6)) # window of 5 \n",
    "total_words = len(wiki_sub)\n",
    "\n",
    "# first create a matrix that counts co-occurences: \n",
    "np_mat = sp.lil_matrix((len(counts),len(counts)))\n",
    "\n",
    "for i in range(len(vocabs)):\n",
    "    if i % 100 == 0:\n",
    "        print(i)\n",
    "    vocab_curr = vocabs[i]\n",
    "    instances = corpus_dict[vocab_curr]\n",
    "    temp = np.array(instances)\n",
    "    context = np.zeros((len(instances),len(radius)))\n",
    "    for j in range(len(radius)):\n",
    "        context[:,j] = temp + radius[j]\n",
    "    context = list(set(list(context.flatten('C'))))\n",
    "    context = [int(k) for k in context if k>=0 and k<total_words]\n",
    "    context_words = [wiki_sub[i] for i in context]\n",
    "    context_words_dict = dict(collections.Counter(context_words))\n",
    "    context_vocabs = list(set(context_words).intersection(set(vocabs)))\n",
    "    if len(context_vocabs)>0:\n",
    "        for word in context_vocabs:\n",
    "            m = vocabs_dict[vocab_curr]\n",
    "            n = vocabs_dict[word]\n",
    "            num = context_words_dict[word]\n",
    "            np_mat[(m,n)] += num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pairs = int(np_mat.sum())\n",
    "marginal_pairs = np_mat.sum(axis=1)\n",
    "outer = np.outer(marginal_pairs,marginal_pairs)\n",
    "np_matd = np_mat.todense()\n",
    "M = (np_matd + 1) * total_pairs\n",
    "M = np.log(np.divide(M,outer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "sM = scipy.sparse.csr_matrix(M) \n",
    "from scipy.sparse.linalg import svds, eigs\n",
    "U,s,V = svds(sM,k=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8124, 50)"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma = np.sqrt(np.diag(s))\n",
    "W = np.matmul(U,sigma)\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_dict = dict(enumerate(vocabs))\n",
    "vocabs_dict = dict((v,k) for k,v in index_dict.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "dist = distance.cdist(W, W, 'euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_check = ['physics','republican','einstein','algebra','fish']\n",
    "word_to_check_indexes = []\n",
    "for word in word_to_check:\n",
    "    word_to_check_indexes.append(vocabs_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3669, 1174, 7181, 4383, 4485]"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_check_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top5_words(index):\n",
    "    word_dist = dist[index,:]\n",
    "    ind = np.argpartition(word_dist,5)[0:5]\n",
    "    res = []\n",
    "    for i in range(5):\n",
    "        res.append(index_dict[ind[i]])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mathematics', 'quantum', 'mechanics', 'physics', 'chemistry'], ['republican', 'democrats', 'presidential', 'candidate', 'electoral'], ['maxwell', 'newton', 'relativity', 'einstein', 'paradox'], ['algebraic', 'algebra', 'topology', 'finite', 'equations'], ['fruit', 'fish', 'meat', 'trees', 'milk']]\n"
     ]
    }
   ],
   "source": [
    "top5s = []\n",
    "for i in word_to_check_indexes:\n",
    "    res = get_top5_words(i)\n",
    "    top5s.append(res)\n",
    "print(top5s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 5 closest words in embedding space all make sense for each of the five words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will explore the following analogies:\n",
    "\n",
    "                                     france:paris = england :?\n",
    "                                     republican:democrat = conservative:?\n",
    "                                     man:woman = male:?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = ['paris','france','england']\n",
    "group2 = ['democrat','republican','conservative']\n",
    "group3 = ['woman','man','male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_analogies(group):\n",
    "    indexes = []\n",
    "    for word in group:\n",
    "        indexes.append(vocabs_dict[word])\n",
    "    v1 = W[indexes[0],:]\n",
    "    v2 = W[indexes[1],:]\n",
    "    v3 = W[indexes[2],:]\n",
    "    v = np.add(np.subtract(v1,v2),v3).reshape(1,50)\n",
    "    dist_v = distance.cdist(W, v, 'euclidean').flatten()\n",
    "    top5_ind = np.argpartition(dist_v,5)[0:5]\n",
    "    res = []\n",
    "    for ind in top5_ind:\n",
    "        res.append(index_dict[ind])\n",
    "    return(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hall', 'london', 'castle', 'boston', 'oxford']"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res1 = get_analogies(group1)\n",
    "res1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conservatives', 'evangelical', 'liberals', 'criticisms', 'reject']"
      ]
     },
     "execution_count": 462,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res2 = get_analogies(group2)\n",
    "res2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['families', 'sex', 'female', 'male', 'couples']"
      ]
     },
     "execution_count": 463,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res3 = get_analogies(group3)\n",
    "res3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our desired outputs are: england to london, conservative to liberal, and male to female, and from our embedding we can get all these outputs from the top 5 nearest embedding vector to the linear combination of the rest of the embedding vectors in the analogies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
